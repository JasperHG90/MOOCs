---
title: "Working with the Coursera Forum Data"
author: "Jasper Ginn"
date: "`r Sys.Date()`"
output: html_document
---

# Introduction

This document outlines several ways to work with the Coursera forum data.

```{r}
# Set global knitr Options
require(knitr)
opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, tidy = FALSE, results = 'asis')
```

# Preparations

```{r}
# Clean wd
rm(list=ls())
# Load packages
packages <- c("RSQLite", "dplyr", "tm", "SnowballC", "lubridate", "ggplot2", "scales")
for(package in packages) if(!require(package, character.only=TRUE)) install.packages(package)
# Load general helper functions
source("/Users/Jasper/Documents/Github.projects/MOOCs/workflow/generic_helper_functions/helper_functions.R")
# Load forum helper functions
source("/Users/Jasper/Documents/Github.projects/MOOCs/coursera_data_dumps/forum_data/forum_helper_functions.R")
# Data locations
data.dir <- "/users/jasper/desktop/TEMP"
files <- paste0(dir_end(data.dir), list.files(data.dir))
```

You can use the "dbQA" ("database Quick Access") function to get all table names residing in a SQLite database, e.g.:

```{r}
# Get all table names
tabs <- dbQA(files[1], show.table=TRUE)
# Print
knitr::kable(data.frame(Table_name = tabs))
```

|Table_name                  |
|:---------------------------|
|activity_log                |
|forum_comments              |
|forum_forums                |
|forum_posts                 |
|forum_reporting             |
|forum_reputation_points     |
|forum_reputation_record     |
|forum_subscribe_forums      |
|forum_subscribe_threads     |
|forum_tags                  |
|forum_tags_threads          |
|forum_threads               |
|kvs_course.forum_readrecord |

Similarly, you can use the dbQA quick function to query a table from the SQLite file, e.g.

```{r}
# We want all forum posts
for.comm <- dbQA(files[1], table = tabs[4], show.table=FALSE)
# Look at structure
str(for.comm)
# Col names
coln <- colnames(for.comm)
outVar <- c("user_id", "edit_time", "user_agent", "text_type")
# Take out variables listed above
for.comm <- for.comm[, which(!sapply(colnames(for.comm), function(x) x %in% outVar))]
```

# Preprocessing

One of the problems with the forum data is that the text is essentially wrapped in html code. Plus, it doesn't behave quite nicely. Before exploring relatively advanced procedures (such as stemming etc.),  we can pre-process this data with some simple convenience functions to strip the html and replace all URLs:

```{r}
# Strip html
for.comm$cleaned_text <- removehtml(for.comm$post_text)
# Take out URLs
for.comm$cleaned_text <- removeurl(for.comm$cleaned_text)
```

Another issue is the character encoding. Essentially, we want the text to be in ASCII format if we are to process it in R:

```{r}
# Convert to ASCII
for.comm$cleaned_text <- iconv(for.comm$cleaned_text, "latin1", "ASCII", sub="")
# Remove double and triple whitespaces
for.comm$cleaned_text <- removeExcessWS(for.comm$cleaned_text)
# Trim leading and trailing whitespace
for.comm$cleaned_text <- removeLeading(for.comm$cleaned_text)
for.comm$cleaned_text <- removeTrailing(for.comm$cleaned_text)
# Trim new lines
for.comm$cleaned_text <- removeNewLine(for.comm$cleaned_text)
# Trim tabs
for.comm$cleaned_text <- removeTabs(for.comm$cleaned_text)
```

By now, the text should be cleaned of most issues. We can convert and format the UNIX timestamps like so:

```{r}
# Convert unix timestamp
for.comm$post_time <- convertunixtm(for.comm$post_time)
# Format to only keep date
for.comm$post_date <- strftime(for.comm$post_time, format="%Y-%m-%d")
```

This way, we can get an idea of when people post comments:

```{r}
# Tally
for.comm.sum <- for.comm %>%
  group_by(post_date) %>%
  tally() %>%
  as.data.frame(.) 
# Convert to date
for.comm.sum$post_date <- as.Date(ymd(for.comm.sum$post_date))
# Plot
ggplot(for.comm.sum, aes(x=post_date, y=n)) +
  geom_line() +
  scale_x_date() +
  theme_bw()
```

![forum comments](https://dl.dropboxusercontent.com/u/38011066/CFI/plots/forumcomments.png)

# Further processing the forum text

We can further process the comments if so desired. This requires the 'tm' and 'SnowballC' packages for text processing:

```{r}
# Turn forum comments into corpus
corp <- VCorpus(VectorSource(for.comm$cleaned_text))
# Show summary
summary(corp)
# Essentially, we have a plain text document for each forum post.
# Inspect
inspect(corp)

# Some more pre-processing : -----

# Replace all capitalized letters with lower-case
corp <- tm_map(corp, tolower)
# Remove punctuation
corp <- tm_map(corp, removePunctuation)
# Strip redundant whitespace
corp <- tm_map(corp, stripWhitespace)
# Remove any numbers present
corp <- tm_map(corp, removeNumbers)
# Remove general stopwords
corp <- tm_map(corp, removeWords, c(stopwords("english"), "one", "can", "also", "use"))
```

We can now apply some functions to stem the data. This can be a bit tricky.

```{r}
# Define a convenience function
conv <- function(x) paste(stemDocument(unlist(strsplit(stripWhitespace(x), " "))), collapse = " ")
# Map stemmer
corp <- tm_map(corp, conv)
# Remove other words
t <- tm_map(corp, removeWords, c("will", 
                              "like", 
                              "new", 
                              "used",
                              "lot",
                              "etc",
                              "get",
                              "put",
                              "see",
                              "week",
                              "tell"))
# Re-corpus
t <- VCorpus(VectorSource(t))
```











