---
title: "Working with the Coursera Forum Data"
author: "Jasper Ginn"
date: "`r Sys.Date()`"
output: html_document
---

# Introduction

This document covers some basic text processing in R using forum comments.

```{r}
# Set global knitr Options
require(knitr)
opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, tidy = FALSE, results = 'asis')
```

# Preparations

```{r}
# Clean wd
rm(list=ls())
# Load packages
packages <- c("RSQLite", "dplyr", "tm", "SnowballC", "lubridate", "ggplot2", "scales", "data.table")
for(package in packages) if(!require(package, character.only=TRUE)) install.packages(package)
# Load general helper functions
source("/Users/Jasper/Documents/Github.projects/MOOCs/workflow/generic_helper_functions/helper_functions.R")
# Load forum helper functions
source("/Users/Jasper/Documents/Github.projects/MOOCs/coursera_data_dumps/forum_data/forum_helper_functions.R")
# Data locations
data.dir <- "/users/jasper/desktop/TEMP"
files <- paste0(dir_end(data.dir), list.files(data.dir))
# Get all table names
tabs <- dbQA(files[1], show.table=TRUE)
# Get comments
for.comm <- dbQA(files[1], table = tabs[4], show.table=FALSE)
# Select
coln <- colnames(for.comm)
outVar <- c("user_id", "edit_time", "user_agent", "text_type")
# Take out variables listed above
for.comm <- for.comm[, which(!sapply(colnames(for.comm), function(x) x %in% outVar))]
```

# Preprocessing

One of the problems with the forum data is that the text is wrapped in html code. Before exploring relatively advanced procedures (such as stemming etc.),  we can pre-process this data with some simple convenience functions to strip the html and replace all URLs:

```{r}
# Strip html
for.comm$cleaned_text <- removehtml(for.comm$post_text)
# Take out URLs
for.comm$cleaned_text <- removeurl(for.comm$cleaned_text)
```

This approach does create an issue with character encoding. We want the text to be in ASCII format if we are to process it in R:

```{r}
# Convert to ASCII
for.comm$cleaned_text <- iconv(for.comm$cleaned_text, "latin1", "ASCII", sub="")
# Trim leading and trailing whitespace for every word. This we do by taking the entire message apart word by word and reconstructing it after having stripped the whitespace issues. This function also takes out punctuation, newlines & tabs.
for.comm$cleaned_text <- sapply(for.comm$cleaned_text, whiteSpaceFix)
```

By now, the text should be cleaned of most issues. 

# Further processing the forum text

We can further process the comments if so desired. This requires the 'tm' and 'SnowballC' packages for text processing:

```{r}
# Turn forum comments into corpus
corp <- VCorpus(VectorSource(for.comm$cleaned_text))
# Show summary
summary(corp)
# Essentially, we have a plain text document for each forum post.
# Inspect
inspect(corp)

# Some more pre-processing : -----

# Replace all capitalized letters with lower-case
corp <- tm_map(corp, tolower)
# Remove punctuation
corp <- tm_map(corp, removePunctuation)
# Strip redundant whitespace
corp <- tm_map(corp, stripWhitespace)
# Remove any numbers present
corp <- tm_map(corp, removeNumbers)
# Remove general stopwords
corp <- tm_map(corp, removeWords, c(stopwords("english"), "one", "can", "also", "use", "name"))
```

We can now apply some functions to stem the data. This can be a bit tricky.

```{r}
# Define a convenience function to stem each word using porter stemmer
conv <- function(x) paste(stemDocument(unlist(strsplit(x, " "))), collapse = " ")
# Map stemmer
corp.t <- tm_map(corp, conv)
# Remove other words
corp.t <- tm_map(corp.t, removeWords, c("will", 
                              "like", 
                              "new", 
                              "used",
                              "lot",
                              "etc",
                              "get",
                              "put",
                              "see",
                              "week",
                              "tell"))
# Re-corpus
corp.t <- VCorpus(VectorSource(corp.t))
```

We can now do some fun stuff like create a wordcloud:

```{r}
# Create wordcloud
require(wordcloud)
wordcloud(corp.t, scale=c(2.5,0.5), max.words=150, rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(5, "Set1"))
```

![wordcloud](https://dl.dropboxusercontent.com/u/38011066/CFI/plots/wordcloud.png)

While good enough for most applications, you can also use slightly more complicated stemmers. The "koRpus" package allows for [lemmanization](http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html). It uses the [treetagger](http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/#Linux) tool for this, which you should install prior to running the following line of code. 

```{r}
# Install koRpus
ifelse(!require(koRpus), install.packages("koRpus"), require(koRpus))
# Use treetagger - this is also a bit tricky. Get install instructions here: http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/#Linux. Also, do not forget to set the path in 'TT.options' to the treetagger installation files.

# Turn forum comments into corpus
corp <- VCorpus(VectorSource(for.comm$cleaned_text))
# Remove punctuation
corp <- tm_map(corp, removePunctuation)
# Strip redundant whitespace
corp <- tm_map(corp, stripWhitespace)
# Remove any numbers present
corp <- tm_map(corp, removeNumbers)
# Remove general stopwords
corp <- tm_map(corp, removeWords, stopwords("english"))
# Function for tagger
tagRes <- function(x) {
  TT <- treetag(x, treetagger="manual", 
          format="obj",
          TT.tknz=FALSE , 
          lang="en",
          TT.options=list(path="/users/jasper/downloads/treeee", 
                          preset="en"))
  # Return data frame
  as.data.frame(TT@TT.res)
  }
# Run
res <- lapply(corp[1:10], function(x){
  temp <- as.character(x)
  tagRes(temp)
})
# To df
res <- rbindlist(res)
# Print
knitr::kable(head(res[,-c(7,8)]))
```

|token      |tag |lemma      | lttr|wclass    |desc                                  |
|:----------|:---|:----------|----:|:---------|:-------------------------------------|
|welcome    |JJ  |welcome    |    7|adjective |Adjective                             |
|course     |NN  |course     |    6|noun      |Noun, singular or mass                |
|leiden     |NN  |<unknown>  |    6|noun      |Noun, singular or mass                |
|university |NN  |university |   10|noun      |Noun, singular or mass                |
|think      |VVP |think      |    5|verb      |Verb, non-3rd person singular present |
|important  |JJ  |important  |    9|adjective |Adjective                             |

However, advanced NLP support is much more developed in [Python](http://www.nltk.org/). 

# Advanced text processing

```{r}
# Create a term document matrix
tdm <- TermDocumentMatrix(t)
# Inspect it
tdm
#inspect(tdm[1:50, 1:12])

# Create df
tdm.df <- as.data.frame(as.matrix(tdm))
# Get me them rowsums!!!!
tdm.df.sum <- data.frame(rowSums(tdm.df))
tdm.df.sum$term <- rownames(tdm.df.sum)
tdm.df.sum <- arrange(tdm.df.sum, desc(rowSums.tdm.df.))

# Create a document term matrix (the transpose of a term document matrix)
#DTM <- DocumentTermMatrix(t)

# Find the most frequently used terms (e.g. terms that have been used 10 times)
freq <- findFreqTerms(tdm, 100)

# Find associations between words (third element is the strength of association)
findAssocs(tdm, "prospective", 0.6)

# Do this for each word in the freq list
b <- lapply(freq, function(x){
  findAssocs(tdm, x, 0.4)
})
# Take out empty vectors
k <- b[!sapply(b, mode) == "list"]
# Create dataframe out of every one
dflist <- function(x){
  z <- x
  #print(z)
  to <- attributes(z[[1]])$dimnames[[1]]
  from <- attributes(z[[1]])$dimnames[[2]]
  l<-unname(unlist(z[[1]]))
  statement <- length(to) == 0
  if(statement == TRUE) {
    return("")
  } else{
    l<-l[1:length(l),]
    #print(from, to, l)
    dt <-as.data.frame(cbind(from, to, l))
    #dt <- as.data.frame(Source = from,
     #                Target = to,
      #               Weight = l)
    return(dt)
  }
}

# Loop
s1 <- dflist(k[1])
for(i in 2:length(k)){
  f <- dflist(k[i])
  if(f != "") {
    s1 <- rbind(s1, f)
  }
}

# Success!!!
colnames(s1) <- c("Source", "Target", "Weight")
# print as csv
write.csv(s1, "/users/jasper/desktop/nodes.csv", row.names=F)

# Rbindlist
require(data.table)
y <- rbindlist(dflist)

f <- dflist(k[2])

b = list()
for(i in 1:10){
  print(dflist(k[i]))
  b <- c(b, dflist(k[i]))
}


print(dflist(k[8]))
k[5]

sapply(k, mode)
k[1]
z <- k[1]
to <- attributes(z[[1]])$dimnames[[1]]
from <- attributes(z[[1]])$dimnames[[2]]
l<-unname(unlist(z[[1]]))
l<-l[1:length(l),]
from <- rep(from, length(l))
dt <- data.frame(Source = from,
           Target = to,
           Weight = l)
# Remove sparse terms
tdm.common <- removeSparseTerms(tdm, 0.95)
dim(tdm.common)
dim(tdm)
inspect(tdm.common)

# Turn TDM matrix into normal matrix
require(slam)
tdm.dense <- as.matrix(tdm.common)
# Look at the matrix
tdm.dense
# Reshape
require(reshape2)
tdm.dense <- melt(tdm.dense, value.name="count")
head(tdm.dense)

# Create the graph (per post)

ggplot(tdm.dense, aes(x = Docs, y = Terms, fill = log10(count))) +
       geom_tile(colour = "white") +
       scale_fill_gradient(high="#FF0000" , low="#FFFFFF")+
       ylab("") +
       theme(panel.background = element_blank()) +
       theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())

#compute document dissimilarity based on the term-document matrix, need to install the proxy package
#it's about distance between 2 documents
install.packages('proxy')
require('proxy')
dis=dist(as.matrix(tdm), method="cosine")
#the dissimilarity is between documents
#visualize the dissimilarity results to matrix, here we are just printing part of the big matrix
as.matrix(dis)[1:10, 1:10]
#visualize the dissimilarity results as a heatmap
heatmap(as.matrix(dis)[1:50, 1:50])
```



